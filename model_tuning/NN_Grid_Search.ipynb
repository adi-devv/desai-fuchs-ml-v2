{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6336d673-9dc6-4ea0-a4c0-853e9f0d1d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from skorch.dataset import Dataset\n",
    "from skorch.helper import predefined_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from skorch import NeuralNetRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle\n",
    "import sys\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm, trange\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler\n",
    "sys.path.append('../')\n",
    "from pyfiles.helpful_functions import InputLogTransformer, OutputLogTransformer, build_neural_network, make_datasets, LDIAModel\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5481f3a7-f295-4ae5-9671-f179211ab255",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = 10 # ADJUST level of gaussian noise added to outputs\n",
    "mod_type = 'nn'\n",
    "description = mod_type + '_noise-' + str(noise)\n",
    "filename = '../datasets/fuchs_v3-2_seed-5_points_25000_noise_' + str(noise) + '.csv'  # CHANGE TO DESIRED DATA FILE\n",
    "df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e6cd259-dd46-40d0-bcbb-740d92ce9f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "input_list = ['Intensity_(W_cm2)', 'Target_Thickness (um)', 'Focal_Distance_(um)'] # independent variables\n",
    "output_list = ['Max_Proton_Energy_(MeV)', 'Total_Proton_Energy_(MeV)', 'Avg_Proton_Energy_(MeV)'] # training outputs\n",
    "\n",
    "X = np.array(df[input_list],dtype=np.float32)\n",
    "y = np.array(df[output_list],dtype=np.float32)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, shuffle = False) # Discard Testing Set for now\n",
    "pct = 25 # Using 5,000/20,000 points in training/validation set for Grid Search\n",
    "len_df = int(len(X_train)*(pct/100))\n",
    "X_train = X_train[0:len_df]\n",
    "y_train = y_train[0:len_df]\n",
    "X_train, y_train, X_val, y_val, input_transformer, output_transformer = make_datasets(X_train, y_train, random_state=42)\n",
    "train_ds = Dataset(X_train, y_train)\n",
    "valid_ds = Dataset(X_val, y_val)\n",
    "print(len(train_ds))\n",
    "print(len(valid_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be01df15-6736-48cc-943e-3afe2bc106fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetRegressor(\n",
    "    module=LDIAModel,\n",
    "    max_epochs = 100,\n",
    "    module__n_hidden=1,\n",
    "    module__n_neurons = 16,\n",
    "    module__activation=nn.LeakyReLU(),\n",
    "    device=device,\n",
    "    criterion = nn.MSELoss(),\n",
    "    batch_size = 32,\n",
    "    optimizer__lr = 1e-3,\n",
    "    iterator_train__shuffle=True,\n",
    "    callbacks=[EarlyStopping(patience=5,monitor='valid_loss'), ('lr_scheduler', LRScheduler(policy='ExponentialLR',gamma=.9))],\n",
    "    verbose=0, \n",
    "    train_split = predefined_split(valid_ds)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a9c1c649-a2de-46de-aaee-7a83aab3505d",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'module__n_hidden':[4],\n",
    "    'module__n_neurons':[16, 32, 64],\n",
    "    'module__activation':[nn.LeakyReLU()],\n",
    "    'optimizer':[optim.Adam],\n",
    "    'callbacks__lr_scheduler__gamma':[0.95],\n",
    "    'batch_size': [256],\n",
    "    'optimizer__lr': [1e-3],\n",
    "    'train_split': [predefined_split(valid_ds)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ffa64df5-41a3-45c6-961c-fbbf2771e3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "[CV 1/5] END batch_size=256, callbacks__lr_scheduler__gamma=0.95, module__activation=LeakyReLU(negative_slope=0.01), module__n_hidden=4, module__n_neurons=16, optimizer=<class 'torch.optim.adam.Adam'>, optimizer__lr=0.001, train_split=functools.partial(<function _make_split at 0x2b30c6a7c670>, valid_ds=<skorch.dataset.Dataset object at 0x2b30d167d040>);, score=-0.009 total time=   5.3s\n",
      "[CV 2/5] END batch_size=256, callbacks__lr_scheduler__gamma=0.95, module__activation=LeakyReLU(negative_slope=0.01), module__n_hidden=4, module__n_neurons=16, optimizer=<class 'torch.optim.adam.Adam'>, optimizer__lr=0.001, train_split=functools.partial(<function _make_split at 0x2b30c6a7c670>, valid_ds=<skorch.dataset.Dataset object at 0x2b30d167d040>);, score=-0.009 total time=   4.9s\n",
      "[CV 3/5] END batch_size=256, callbacks__lr_scheduler__gamma=0.95, module__activation=LeakyReLU(negative_slope=0.01), module__n_hidden=4, module__n_neurons=16, optimizer=<class 'torch.optim.adam.Adam'>, optimizer__lr=0.001, train_split=functools.partial(<function _make_split at 0x2b30c6a7c670>, valid_ds=<skorch.dataset.Dataset object at 0x2b30d167d040>);, score=-0.008 total time=   4.3s\n",
      "[CV 4/5] END batch_size=256, callbacks__lr_scheduler__gamma=0.95, module__activation=LeakyReLU(negative_slope=0.01), module__n_hidden=4, module__n_neurons=16, optimizer=<class 'torch.optim.adam.Adam'>, optimizer__lr=0.001, train_split=functools.partial(<function _make_split at 0x2b30c6a7c670>, valid_ds=<skorch.dataset.Dataset object at 0x2b30d167d040>);, score=-0.007 total time=   5.5s\n",
      "[CV 5/5] END batch_size=256, callbacks__lr_scheduler__gamma=0.95, module__activation=LeakyReLU(negative_slope=0.01), module__n_hidden=4, module__n_neurons=16, optimizer=<class 'torch.optim.adam.Adam'>, optimizer__lr=0.001, train_split=functools.partial(<function _make_split at 0x2b30c6a7c670>, valid_ds=<skorch.dataset.Dataset object at 0x2b30d167d040>);, score=-0.009 total time=   5.4s\n",
      "[CV 1/5] END batch_size=256, callbacks__lr_scheduler__gamma=0.95, module__activation=LeakyReLU(negative_slope=0.01), module__n_hidden=4, module__n_neurons=32, optimizer=<class 'torch.optim.adam.Adam'>, optimizer__lr=0.001, train_split=functools.partial(<function _make_split at 0x2b30c6a7c670>, valid_ds=<skorch.dataset.Dataset object at 0x2b30d167d040>);, score=-0.006 total time=   3.5s\n",
      "[CV 2/5] END batch_size=256, callbacks__lr_scheduler__gamma=0.95, module__activation=LeakyReLU(negative_slope=0.01), module__n_hidden=4, module__n_neurons=32, optimizer=<class 'torch.optim.adam.Adam'>, optimizer__lr=0.001, train_split=functools.partial(<function _make_split at 0x2b30c6a7c670>, valid_ds=<skorch.dataset.Dataset object at 0x2b30d167d040>);, score=-0.006 total time=   2.7s\n",
      "[CV 3/5] END batch_size=256, callbacks__lr_scheduler__gamma=0.95, module__activation=LeakyReLU(negative_slope=0.01), module__n_hidden=4, module__n_neurons=32, optimizer=<class 'torch.optim.adam.Adam'>, optimizer__lr=0.001, train_split=functools.partial(<function _make_split at 0x2b30c6a7c670>, valid_ds=<skorch.dataset.Dataset object at 0x2b30d167d040>);, score=-0.006 total time=   3.2s\n",
      "[CV 4/5] END batch_size=256, callbacks__lr_scheduler__gamma=0.95, module__activation=LeakyReLU(negative_slope=0.01), module__n_hidden=4, module__n_neurons=32, optimizer=<class 'torch.optim.adam.Adam'>, optimizer__lr=0.001, train_split=functools.partial(<function _make_split at 0x2b30c6a7c670>, valid_ds=<skorch.dataset.Dataset object at 0x2b30d167d040>);, score=-0.006 total time=   2.6s\n",
      "[CV 5/5] END batch_size=256, callbacks__lr_scheduler__gamma=0.95, module__activation=LeakyReLU(negative_slope=0.01), module__n_hidden=4, module__n_neurons=32, optimizer=<class 'torch.optim.adam.Adam'>, optimizer__lr=0.001, train_split=functools.partial(<function _make_split at 0x2b30c6a7c670>, valid_ds=<skorch.dataset.Dataset object at 0x2b30d167d040>);, score=-0.006 total time=   3.5s\n",
      "[CV 1/5] END batch_size=256, callbacks__lr_scheduler__gamma=0.95, module__activation=LeakyReLU(negative_slope=0.01), module__n_hidden=4, module__n_neurons=64, optimizer=<class 'torch.optim.adam.Adam'>, optimizer__lr=0.001, train_split=functools.partial(<function _make_split at 0x2b30c6a7c670>, valid_ds=<skorch.dataset.Dataset object at 0x2b30d167d040>);, score=-0.006 total time=   1.6s\n",
      "[CV 2/5] END batch_size=256, callbacks__lr_scheduler__gamma=0.95, module__activation=LeakyReLU(negative_slope=0.01), module__n_hidden=4, module__n_neurons=64, optimizer=<class 'torch.optim.adam.Adam'>, optimizer__lr=0.001, train_split=functools.partial(<function _make_split at 0x2b30c6a7c670>, valid_ds=<skorch.dataset.Dataset object at 0x2b30d167d040>);, score=-0.006 total time=   1.9s\n",
      "[CV 3/5] END batch_size=256, callbacks__lr_scheduler__gamma=0.95, module__activation=LeakyReLU(negative_slope=0.01), module__n_hidden=4, module__n_neurons=64, optimizer=<class 'torch.optim.adam.Adam'>, optimizer__lr=0.001, train_split=functools.partial(<function _make_split at 0x2b30c6a7c670>, valid_ds=<skorch.dataset.Dataset object at 0x2b30d167d040>);, score=-0.005 total time=   3.2s\n",
      "[CV 4/5] END batch_size=256, callbacks__lr_scheduler__gamma=0.95, module__activation=LeakyReLU(negative_slope=0.01), module__n_hidden=4, module__n_neurons=64, optimizer=<class 'torch.optim.adam.Adam'>, optimizer__lr=0.001, train_split=functools.partial(<function _make_split at 0x2b30c6a7c670>, valid_ds=<skorch.dataset.Dataset object at 0x2b30d167d040>);, score=-0.006 total time=   2.4s\n",
      "[CV 5/5] END batch_size=256, callbacks__lr_scheduler__gamma=0.95, module__activation=LeakyReLU(negative_slope=0.01), module__n_hidden=4, module__n_neurons=64, optimizer=<class 'torch.optim.adam.Adam'>, optimizer__lr=0.001, train_split=functools.partial(<function _make_split at 0x2b30c6a7c670>, valid_ds=<skorch.dataset.Dataset object at 0x2b30d167d040>);, score=-0.005 total time=   2.1s\n",
      "Best: -0.005617 using {'batch_size': 256, 'callbacks__lr_scheduler__gamma': 0.95, 'module__activation': LeakyReLU(negative_slope=0.01), 'module__n_hidden': 4, 'module__n_neurons': 64, 'optimizer': <class 'torch.optim.adam.Adam'>, 'optimizer__lr': 0.001, 'train_split': functools.partial(<function _make_split at 0x2b30c6a7c670>, valid_ds=<skorch.dataset.Dataset object at 0x2b30d167d040>)}\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, refit=False, n_jobs=1, cv=5,scoring='neg_mean_squared_error',verbose=3)\n",
    "grid_result = grid.fit(train_ds.X, train_ds.y)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "# for mean, stdev, param in zip(means, stds, params):\n",
    "#     print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a8ed424f-6c91-4103-aabd-3d8fc4e32a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: -0.005617 using \n",
      "{'batch_size': 256, 'callbacks__lr_scheduler__gamma': 0.95, 'module__activation': LeakyReLU(negative_slope=0.01), 'module__n_hidden': 4, 'module__n_neurons': 64, 'optimizer': <class 'torch.optim.adam.Adam'>, 'optimizer__lr': 0.001, 'train_split': functools.partial(<function _make_split at 0x2b30c6a7c670>, valid_ds=<skorch.dataset.Dataset object at 0x2b30d167d040>)}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Score: %f using \\n%s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "# idxs = np.argsort(means)\n",
    "# idx = 1\n",
    "# print(means[idxs[-idx]])\n",
    "# print(stds[idxs[-idx]])\n",
    "# print(params[idxs[-idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "79133472-1454-464c-9b1b-36d08cad9940",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nn_cv_results/grid_search_4.pkl', 'wb') as file:\n",
    "    pickle.dump(grid_result.cv_results_, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55091fa-0204-4a5d-b1df-43448df0a063",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env [pytorch-env]",
   "language": "python",
   "name": "conda_pytorch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
