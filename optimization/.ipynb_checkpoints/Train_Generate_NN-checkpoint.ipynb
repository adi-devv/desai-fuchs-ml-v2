{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "226055e3-d404-45a1-8cdf-1e0fefd38bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from skorch import NeuralNet, NeuralNetRegressor\n",
    "from sklearn.metrics import mean_squared_error as mse, mean_absolute_percentage_error as mape\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm, trange\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler\n",
    "from skorch.dataset import Dataset\n",
    "from skorch.helper import predefined_split\n",
    "import joblib\n",
    "from skorch.callbacks import Callback\n",
    "import time\n",
    "#from helpful_functions import InputLogTransformer, OutputLogTransformer, build_neural_network, make_datasets\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def log_transform(X): # transforms first column of data.  Alternativly, this could be implemented via operations on DataFrames\n",
    "    X1 = X.copy()\n",
    "    X1[:, 0] = np.log(X1[:, 0])\n",
    "    return X1\n",
    "def log_inverse(X):\n",
    "    X1 = X.copy()\n",
    "    X1[:, 0] = np.exp(X1[:,0])\n",
    "    return X1\n",
    "\n",
    "class OutputLogTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self._estimator = StandardScaler()\n",
    "    def fit(self, y):\n",
    "        y_copy = y.copy()\n",
    "        y_copy = np.log(y_copy)\n",
    "        self._estimator.fit(y_copy)\n",
    "        \n",
    "        return self\n",
    "    def transform(self, y):\n",
    "        y_copy = y.copy()\n",
    "        y_copy = np.log(y_copy)\n",
    "        return self._estimator.transform(y_copy)\n",
    "    def inverse_transform(self, y):\n",
    "        y_copy = y.copy()\n",
    "        y_reverse = np.exp(self._estimator.inverse_transform(y_copy))\n",
    "        \n",
    "        return y_reverse\n",
    "\n",
    "class InputLogTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self._estimator = StandardScaler()\n",
    "    def fit(self, X):\n",
    "        X_copy = X.copy()\n",
    "        X_copy = log_transform(X_copy)\n",
    "        self._estimator.fit(X_copy)\n",
    "        \n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        X_copy = log_transform(X_copy)\n",
    "        return self._estimator.transform(X_copy)\n",
    "    def inverse_transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        X_reverse = log_inverse(self._estimator.inverse_transform(X_copy))\n",
    "        \n",
    "        return X_reverse\n",
    "    \n",
    "class LDIAModel(nn.Module):\n",
    "    '''\n",
    "    Perceptron Model of variable architecture for hyperparameter tuning\n",
    "    '''\n",
    "    def __init__(self, n_hidden = 1,n_neurons=64,activation=nn.LeakyReLU()):\n",
    "        super().__init__()\n",
    "        self.norms = []\n",
    "        self.layers = []\n",
    "        self.acts = []\n",
    "        self.norm0 = nn.BatchNorm1d(3)\n",
    "        self.layer0 = nn.Linear(3,n_neurons)\n",
    "        for i in range(1,n_hidden+1):\n",
    "            self.norms.append(nn.BatchNorm1d(n_neurons))\n",
    "            self.acts.append(activation)\n",
    "            self.add_module(f\"norm{i}\", self.norms[-1])\n",
    "            self.add_module(f\"act{i}\", self.acts[-1])\n",
    "            if (i != n_hidden):\n",
    "                self.layers.append(nn.Linear(n_neurons, n_neurons))\n",
    "                self.add_module(f\"layer{i}\", self.layers[-1])\n",
    "        self.output = nn.Linear(n_neurons, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "          Forward pass\n",
    "        '''\n",
    "        x = self.layer0(self.norm0(x))\n",
    "        for norm, layer, act in zip(self.norms, self.layers, self.acts):\n",
    "            x = act(layer(norm(x)))\n",
    "        return self.output(x)\n",
    "    \n",
    "def build_neural_network(max_epochs=100, n_hidden=3, n_neurons=32, activation=nn.LeakyReLU(), device=torch.device('cuda'),loss_fn=nn.MSELoss(), optimizer=optim.Adam, lr=1e-2, shuffled=True, batch_size=1024, patience=5, gamma=0.95,valid_ds=None,compiled=False):\n",
    "    return NeuralNetRegressor(\n",
    "    module=LDIAModel,\n",
    "    max_epochs = max_epochs,\n",
    "    module__n_hidden=n_hidden,\n",
    "    module__n_neurons = n_neurons,\n",
    "    module__activation=activation,\n",
    "    device=device,\n",
    "    criterion = loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    optimizer__lr = lr,\n",
    "    iterator_train__shuffle=shuffled,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[('early_stopping', EarlyStopping(patience=patience,monitor='valid_loss')),\n",
    "    ('lr_scheduler', LRScheduler(policy='ExponentialLR',gamma=gamma))],\n",
    "    train_split=predefined_split(valid_ds),\n",
    "    compile = compiled \n",
    ")\n",
    "\n",
    "def make_datasets(X, y, *, train_size=0.8, random_state=42):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=train_size,random_state=random_state)\n",
    "    input_transformer = InputLogTransformer()\n",
    "    output_transformer = OutputLogTransformer()\n",
    "    X_train = input_transformer.fit_transform(X_train)\n",
    "    X_val = input_transformer.transform(X_val)\n",
    "    y_train = output_transformer.fit_transform(y_train)\n",
    "    y_val = output_transformer.transform(y_val)\n",
    "    #train_ds = Dataset(X_train, y_train)\n",
    "    #valid_ds = Dataset(X_val, y_val)\n",
    "    return X_train, y_train, X_val, y_val, input_transformer, output_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5c450b2-c6b7-43e7-9a1c-31928cd27594",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = 30 # ADJUST level of gaussian noise added to outputs\n",
    "mod_type = 'nn'\n",
    "description = mod_type + '_noise-' + str(noise)\n",
    "filename = '../datasets/fuchs_v3-2_seed-5_points_25000_noise_' + str(noise) + '.csv'  # CHANGE TO DESIRED DATA FILE\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "input_list = ['Intensity_(W_cm2)', 'Target_Thickness (um)', 'Focal_Distance_(um)'] # independent variables\n",
    "output_list = ['Max_Proton_Energy_(MeV)', 'Total_Proton_Energy_(MeV)', 'Avg_Proton_Energy_(MeV)',\n",
    "               'Max_Proton_Energy_Exact_(MeV)', 'Total_Proton_Energy_Exact_(MeV)', 'Avg_Proton_Energy_Exact_(MeV)'] # training outputs\n",
    "X = np.array(df[input_list].copy(), dtype=np.float32)\n",
    "y = np.array(df[output_list].copy(), dtype=np.float32)\n",
    "    \n",
    "train_split = 0.8 # Reserve 80% of entire dataset for training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_split, shuffle = False)\n",
    "y_train = y_train[:, 0:3]\n",
    "\n",
    "pct = 5 # e.g. 10% is 2,000 points training\n",
    "len_df = int(len(X_train)*(pct/100))\n",
    "X_train = X_train[0:len_df]\n",
    "y_train = y_train[0:len_df]\n",
    "\n",
    "X_train_scaled, y_train_scaled, X_val_scaled, y_val_scaled, input_transformer, output_transformer = make_datasets(X_train, y_train, random_state=42)\n",
    "train_ds = Dataset(X_train_scaled, y_train_scaled)\n",
    "valid_ds = Dataset(X_val_scaled, y_val_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7112d0c5-5df0-494b-ac4f-a5b08ad1f3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1        \u001b[36m0.9375\u001b[0m        \u001b[32m0.9308\u001b[0m  0.0010  0.3510\n",
      "      2        \u001b[36m0.5557\u001b[0m        \u001b[32m0.7611\u001b[0m  0.0009  0.3107\n",
      "      3        \u001b[36m0.3278\u001b[0m        \u001b[32m0.5480\u001b[0m  0.0009  0.4870\n",
      "      4        \u001b[36m0.2112\u001b[0m        \u001b[32m0.3547\u001b[0m  0.0009  0.3082\n",
      "      5        \u001b[36m0.1467\u001b[0m        \u001b[32m0.2165\u001b[0m  0.0008  0.3912\n",
      "      6        \u001b[36m0.1054\u001b[0m        \u001b[32m0.1344\u001b[0m  0.0008  0.5046\n",
      "      7        \u001b[36m0.0813\u001b[0m        \u001b[32m0.0925\u001b[0m  0.0007  0.3076\n",
      "      8        \u001b[36m0.0669\u001b[0m        \u001b[32m0.0725\u001b[0m  0.0007  0.5896\n",
      "      9        \u001b[36m0.0558\u001b[0m        \u001b[32m0.0614\u001b[0m  0.0007  0.4015\n",
      "     10        0.0568        \u001b[32m0.0527\u001b[0m  0.0006  0.3917\n",
      "     11        \u001b[36m0.0503\u001b[0m        \u001b[32m0.0465\u001b[0m  0.0006  0.5037\n",
      "     12        \u001b[36m0.0491\u001b[0m        \u001b[32m0.0425\u001b[0m  0.0006  0.3923\n",
      "     13        \u001b[36m0.0430\u001b[0m        \u001b[32m0.0408\u001b[0m  0.0005  0.6929\n",
      "     14        0.0447        \u001b[32m0.0403\u001b[0m  0.0005  0.3096\n",
      "     15        0.0436        \u001b[32m0.0401\u001b[0m  0.0005  0.3139\n",
      "     16        \u001b[36m0.0423\u001b[0m        0.0413  0.0005  1.3012\n",
      "     17        0.0503        0.0416  0.0004  0.9044\n",
      "     18        0.0495        0.0407  0.0004  0.8069\n",
      "     19        \u001b[36m0.0409\u001b[0m        \u001b[32m0.0398\u001b[0m  0.0004  0.8967\n",
      "     20        0.0451        \u001b[32m0.0395\u001b[0m  0.0004  0.7989\n",
      "     21        \u001b[36m0.0403\u001b[0m        0.0396  0.0004  0.7049\n",
      "     22        0.0440        0.0398  0.0003  0.8035\n",
      "     23        0.0509        0.0423  0.0003  0.9060\n",
      "     24        0.0463        0.0411  0.0003  0.7941\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "0.2552827\n",
      "0.2621808\n",
      "0.25227597\n"
     ]
    }
   ],
   "source": [
    "num_inputs = 3\n",
    "num_outputs = 3\n",
    "n_hidden = 3\n",
    "n_neurons = 64\n",
    "train_split = 0.8 # Reserve 80% of entire dataset for training\n",
    "batch_size = 256\n",
    "learning_rate = 1e-2\n",
    "γ = 0.95\n",
    "max_epochs = 100\n",
    "lr=1e-3\n",
    "\n",
    "nn_model = build_neural_network(max_epochs=max_epochs, \n",
    "                                 n_hidden=n_hidden, \n",
    "                                 n_neurons=n_neurons, \n",
    "                                 activation=nn.LeakyReLU(), \n",
    "                                 device=device,loss_fn=nn.MSELoss(), \n",
    "                                 optimizer=optim.Adam, \n",
    "                                 lr=lr, \n",
    "                                 shuffled=True, \n",
    "                                 batch_size=batch_size, \n",
    "                                 patience=5, \n",
    "                                 gamma=γ, \n",
    "                                 valid_ds=valid_ds)\n",
    "    \n",
    "nn_model.fit(train_ds, y=None)\n",
    "\n",
    "y_train_predict = nn_model.predict(X_train_scaled)\n",
    "y_train_predict_unscaled = output_transformer.inverse_transform(y_train_predict)\n",
    "\n",
    "# Corrections due to Log Scaling\n",
    "y_train = output_transformer.inverse_transform(y_train_scaled)\n",
    "correction_factor = np.mean(y_train/y_train_predict_unscaled, axis=0) \n",
    "y_train_predict_corrected = y_train_predict_unscaled*correction_factor\n",
    "for j in range(num_outputs):\n",
    "    print(mape(y_train[:, j], y_train_predict_corrected[:, j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87ee8a0c-4340-499c-adc6-cdb9c619f39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X):\n",
    "    X_scaled = input_transformer.transform(X.copy().reshape(-1, 3))\n",
    "    energies = output_transformer.inverse_transform(nn_model.predict(X_scaled))\n",
    "    E_max = energies[:, 0]\n",
    "    E_tot = energies[:, 1]\n",
    "    E_avg = energies[:, 2]\n",
    "\n",
    "    return E_max, E_tot, E_avg\n",
    "\n",
    "def generate_random_points(bounds, n):\n",
    "    np.random.seed(0)\n",
    "    points = []\n",
    "    for bound in bounds:\n",
    "        points.append(np.random.uniform(bound[0], bound[1], n))\n",
    "    return np.array(points, dtype=np.float32).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01815a08-6e05-4a4a-922c-c756035be3c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intensity</th>\n",
       "      <th>Thickness</th>\n",
       "      <th>Offset</th>\n",
       "      <th>E Max</th>\n",
       "      <th>E Tot</th>\n",
       "      <th>E Avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000e+19</td>\n",
       "      <td>6.132362</td>\n",
       "      <td>8.115185</td>\n",
       "      <td>0.165974</td>\n",
       "      <td>50991012.0</td>\n",
       "      <td>0.047128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000e+19</td>\n",
       "      <td>0.595605</td>\n",
       "      <td>4.760840</td>\n",
       "      <td>1.374366</td>\n",
       "      <td>434532800.0</td>\n",
       "      <td>0.256355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000e+19</td>\n",
       "      <td>5.020349</td>\n",
       "      <td>5.231560</td>\n",
       "      <td>0.305178</td>\n",
       "      <td>101692856.0</td>\n",
       "      <td>0.082343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000e+19</td>\n",
       "      <td>7.233319</td>\n",
       "      <td>2.505206</td>\n",
       "      <td>0.219004</td>\n",
       "      <td>63006216.0</td>\n",
       "      <td>0.054922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000e+19</td>\n",
       "      <td>0.917767</td>\n",
       "      <td>6.050430</td>\n",
       "      <td>1.153874</td>\n",
       "      <td>358303776.0</td>\n",
       "      <td>0.215709</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Intensity  Thickness    Offset     E Max        E Tot     E Avg\n",
       "0  1.000000e+19   6.132362  8.115185  0.165974   50991012.0  0.047128\n",
       "1  1.000000e+19   0.595605  4.760840  1.374366  434532800.0  0.256355\n",
       "2  1.000000e+19   5.020349  5.231560  0.305178  101692856.0  0.082343\n",
       "3  1.000000e+19   7.233319  2.505206  0.219004   63006216.0  0.054922\n",
       "4  1.000000e+19   0.917767  6.050430  1.153874  358303776.0  0.215709"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bounds = [(1e19, 1e19), (0.5, 10.0), (0, 10.0)]\n",
    "n_points = 1000\n",
    "points = generate_random_points(bounds, n_points)\n",
    "Emax, Etot, Eavg = model(points)\n",
    "output_df = pd.DataFrame(columns=['Intensity', 'Thickness', 'Offset', 'E Max', 'E Tot', 'E Avg'])\n",
    "output_df['Intensity'] = points[:, 0]\n",
    "output_df['Thickness'] = points[:, 1]\n",
    "output_df['Offset'] = points[:, 2]\n",
    "output_df['E Max'] = Emax\n",
    "output_df['E Tot'] = Etot\n",
    "output_df['E Avg'] = Eavg\n",
    "output_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91ed3bc7-0b7a-42fe-bfdd-11c001cd6161",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.to_csv('predictions_dfs/{}_noise={}_train_pts={}.csv'.format(mod_type, noise, len_df), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f0eb9a-58e9-4e34-8c93-38e34e118012",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
