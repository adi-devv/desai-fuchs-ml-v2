{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f52e24e4-f7d0-448e-89d8-e8fe9c6ac530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from collections.abc import Iterable\n",
    "import time\n",
    "import math \n",
    "import sys\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import os\n",
    "import gpytorch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"The input matches the stored training data. Did you forget to call model.train()?\") \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error as mse, mean_absolute_percentage_error as mape\n",
    "import gc\n",
    "from tqdm import trange, tqdm\n",
    "dataType = torch.float64\n",
    "torch.set_default_dtype(dataType)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "class Exact_GP(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(Exact_GP, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cafba04-0de1-4901-8c0d-0a09f12278be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train ds length:  10000\n",
      "torch.float64\n"
     ]
    }
   ],
   "source": [
    "noise = 30 # ADJUST level of gaussian noise added to outputs\n",
    "mod_type = 'gpr'\n",
    "description = mod_type + '_noise-' + str(noise)\n",
    "filename = '../datasets/fuchs_v3-2_seed-5_points_25000_noise_' + str(noise) + '.csv'  # CHANGE TO DESIRED DATA FILE\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "input_list = ['Intensity_(W_cm2)', 'Target_Thickness (um)', 'Focal_Distance_(um)'] # independent variables\n",
    "output_list = ['Max_Proton_Energy_(MeV)', 'Total_Proton_Energy_(MeV)', 'Avg_Proton_Energy_(MeV)',\n",
    "               'Max_Proton_Energy_Exact_(MeV)', 'Total_Proton_Energy_Exact_(MeV)', 'Avg_Proton_Energy_Exact_(MeV)'] # training outputs\n",
    "X = df[input_list].copy()\n",
    "y = df[output_list].copy()\n",
    "X[X.columns[0]] = np.log(X[X.columns[0]]) # Apply log scaling to intensity\n",
    "for col in y.columns:\n",
    "    y[col] = np.log(y[col]) # Apply log scaling to energy\n",
    "\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, shuffle = False)\n",
    "y_train = y_train[:, 0:3]\n",
    "pct = 50 # Using all 20,000 points in training/validation set\n",
    "len_df = int(len(X_train)*(pct/100))\n",
    "X_train = X_train[0:len_df]\n",
    "y_train = y_train[0:len_df]\n",
    "\n",
    "ss_in = StandardScaler()\n",
    "ss_in.fit(X_train)\n",
    "X_train_norm = ss_in.transform(X_train)\n",
    "\n",
    "ss_out = StandardScaler()\n",
    "ss_out.fit(y_train)\n",
    "y_train_norm = ss_out.transform(y_train)\n",
    "\n",
    "X_train_norm = torch.tensor(X_train_norm, dtype=dataType).to(device)\n",
    "y_train_norm = torch.tensor(y_train_norm, dtype=dataType).to(device)\n",
    "\n",
    "print('train ds length: ', len(X_train_norm))\n",
    "print(X_train_norm.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22ae6fd3-1357-4cc8-994a-42ba462e4f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25488250935377693\n",
      "0.2551784720498006\n",
      "0.25473899160757046\n"
     ]
    }
   ],
   "source": [
    "num_inputs = 3\n",
    "num_outputs = 3\n",
    "num_epochs = 30\n",
    "lr = 2e-1\n",
    "\n",
    "likelihoods = []\n",
    "gprs = []\n",
    "correction_factor = []\n",
    "for j in range(num_outputs):\n",
    "    # Train GPR\n",
    "    likelihoods.append(gpytorch.likelihoods.GaussianLikelihood().to(device))\n",
    "    gprs.append(Exact_GP(X_train_norm, y_train_norm[:, j], likelihoods[j]).to(device))\n",
    "\n",
    "    gprs[j].train()\n",
    "    likelihoods[j].train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(gprs[j].parameters(), lr=lr)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihoods[j], gprs[j])\n",
    "\n",
    "    current_it = 0\n",
    "\n",
    "    while(current_it < num_epochs):\n",
    "        # Zero the Gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform Forward Pass\n",
    "        model_output = gprs[j](X_train_norm)\n",
    "\n",
    "        # Compute Loss\n",
    "        loss = -mll(model_output, y_train_norm[:, j])\n",
    "\n",
    "        # Perform Backward Pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Clear cache\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        current_it += 1\n",
    "\n",
    "        # Optimization\n",
    "        optimizer.step()\n",
    "\n",
    "    gprs[j].eval()\n",
    "    likelihoods[j].eval()\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        # Make Predictions\n",
    "        pred_dist_train = likelihoods[j](gprs[j](X_train_norm))\n",
    "        y_train_predict = pred_dist_train.mean\n",
    "        y_train_predict_unscaled = np.exp(ss_out.inverse_transform(y_train_predict.cpu().detach().numpy().reshape(-1, 1).repeat(3, 1)))[:, j]\n",
    "\n",
    "        # Corrections due to Log Scaling\n",
    "        correction_factor.append(np.mean(np.exp(y_train[:, j])/y_train_predict_unscaled))\n",
    "        y_train_predict_corrected = y_train_predict_unscaled*correction_factor[j]\n",
    "        print(mape(np.exp(y_train[:, j]), y_train_predict_corrected))\n",
    "        \n",
    "    # Append output to input for next iteration in chained output regression\n",
    "    X_train_norm = torch.concat([X_train_norm, y_train_predict.reshape(-1, 1)], axis=1)\n",
    "        \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa7c7cd9-111b-4473-ad37-9864a4dd7da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X):\n",
    "    X_mod = X.copy().reshape(-1, 3)\n",
    "    X_mod[:,0] = np.log(X_mod[:,0])\n",
    "    X_scaled = torch.Tensor(ss_in.transform(X_mod)).to(device)\n",
    "    Energies = []\n",
    "    for j in range(num_outputs):\n",
    "        y_predict = likelihoods[j](gprs[j](X_scaled)).mean\n",
    "        Energies.append(np.exp(ss_out.inverse_transform(y_predict.detach().cpu().numpy().reshape(-1, 1).repeat(3, 1) ) )[:, j]*correction_factor[j])\n",
    "        X_scaled = torch.concat([X_scaled, y_predict.reshape(-1, 1)], axis=1)\n",
    "    E_max, E_tot, E_avg = Energies # Max Energy, Total Energy, Average Energy\n",
    "    return (E_max, E_tot, E_avg)\n",
    "\n",
    "def generate_random_points(bounds, n):\n",
    "    np.random.seed(0)\n",
    "    points = []\n",
    "    for bound in bounds:\n",
    "        points.append(np.random.uniform(bound[0], bound[1], n))\n",
    "    return np.array(points).transpose()\n",
    "\n",
    "def split_points(points, batch_size):\n",
    "    i = 0\n",
    "    split_points = []\n",
    "    while i < len(points):\n",
    "        if i + batch_size > len(points):\n",
    "            split_points.append(points[i:])\n",
    "        else:\n",
    "            split_points.append(points[i:i+batch_size])\n",
    "        i += batch_size\n",
    "    return split_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "499a7773-3dbc-4968-b00b-c9d1f59a562b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intensity</th>\n",
       "      <th>Thickness</th>\n",
       "      <th>Offset</th>\n",
       "      <th>E Max</th>\n",
       "      <th>E Tot</th>\n",
       "      <th>E Avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000e+19</td>\n",
       "      <td>5.584942</td>\n",
       "      <td>2.177699</td>\n",
       "      <td>0.373143</td>\n",
       "      <td>1.114780e+08</td>\n",
       "      <td>0.092978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000e+19</td>\n",
       "      <td>9.088420</td>\n",
       "      <td>5.872497</td>\n",
       "      <td>0.134569</td>\n",
       "      <td>3.054076e+07</td>\n",
       "      <td>0.036712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000e+19</td>\n",
       "      <td>5.272767</td>\n",
       "      <td>9.876133</td>\n",
       "      <td>0.176680</td>\n",
       "      <td>3.821466e+07</td>\n",
       "      <td>0.044813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000e+19</td>\n",
       "      <td>1.458265</td>\n",
       "      <td>9.508670</td>\n",
       "      <td>0.549921</td>\n",
       "      <td>1.354106e+08</td>\n",
       "      <td>0.111632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000e+19</td>\n",
       "      <td>5.512029</td>\n",
       "      <td>3.429715</td>\n",
       "      <td>0.350828</td>\n",
       "      <td>1.012088e+08</td>\n",
       "      <td>0.087357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Intensity  Thickness    Offset     E Max         E Tot     E Avg\n",
       "0  1.000000e+19   5.584942  2.177699  0.373143  1.114780e+08  0.092978\n",
       "1  1.000000e+19   9.088420  5.872497  0.134569  3.054076e+07  0.036712\n",
       "2  1.000000e+19   5.272767  9.876133  0.176680  3.821466e+07  0.044813\n",
       "3  1.000000e+19   1.458265  9.508670  0.549921  1.354106e+08  0.111632\n",
       "4  1.000000e+19   5.512029  3.429715  0.350828  1.012088e+08  0.087357"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for j in range(num_outputs):\n",
    "    gprs[j].eval()\n",
    "    likelihoods[j].eval()\n",
    "bounds = [(1e19, 1e19), (0.5, 10.0), (0, 10.0)]\n",
    "n_points = 100000\n",
    "points = generate_random_points(bounds, n_points)\n",
    "points_array = split_points(points, 1000)\n",
    "output_df = pd.DataFrame(columns=['Intensity', 'Thickness', 'Offset', 'E Max', 'E Tot', 'E Avg'])\n",
    "for batch in points_array:\n",
    "    Emax, Etot, Eavg = model(batch)\n",
    "    batch_df = pd.DataFrame(columns=['Intensity', 'Thickness', 'Offset', 'E Max', 'E Tot', 'E Avg'])\n",
    "    batch_df['Intensity'] = batch[:, 0]\n",
    "    batch_df['Thickness'] = batch[:, 1]\n",
    "    batch_df['Offset'] = batch[:, 2]\n",
    "    batch_df['E Max'] = Emax\n",
    "    batch_df['E Tot'] = Etot\n",
    "    batch_df['E Avg'] = Eavg\n",
    "    output_df = pd.concat([output_df, batch_df], ignore_index=True)\n",
    "\n",
    "output_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a70bee50-4225-4728-b848-ed4200bf38ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.to_csv('predictions_dfs/{}_noise={}_train_pts={}.csv'.format(mod_type, noise, len_df), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db06f2aa-ce95-47a6-9de5-09f74e208f98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env [pytorch-env]",
   "language": "python",
   "name": "conda_pytorch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
