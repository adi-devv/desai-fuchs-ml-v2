{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "226055e3-d404-45a1-8cdf-1e0fefd38bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from skorch import NeuralNet, NeuralNetRegressor\n",
    "from sklearn.metrics import mean_squared_error as mse, mean_absolute_percentage_error as mape\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm, trange\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from skorch.callbacks import EarlyStopping, LRScheduler\n",
    "from skorch.dataset import Dataset\n",
    "from skorch.helper import predefined_split\n",
    "import joblib\n",
    "from skorch.callbacks import Callback\n",
    "import time\n",
    "#from helpful_functions import InputLogTransformer, OutputLogTransformer, build_neural_network, make_datasets\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def log_transform(X): # transforms first column of data.  Alternativly, this could be implemented via operations on DataFrames\n",
    "    X1 = X.copy()\n",
    "    X1[:, 0] = np.log(X1[:, 0])\n",
    "    return X1\n",
    "def log_inverse(X):\n",
    "    X1 = X.copy()\n",
    "    X1[:, 0] = np.exp(X1[:,0])\n",
    "    return X1\n",
    "\n",
    "class OutputLogTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self._estimator = StandardScaler()\n",
    "    def fit(self, y):\n",
    "        y_copy = y.copy()\n",
    "        y_copy = np.log(y_copy)\n",
    "        self._estimator.fit(y_copy)\n",
    "        \n",
    "        return self\n",
    "    def transform(self, y):\n",
    "        y_copy = y.copy()\n",
    "        y_copy = np.log(y_copy)\n",
    "        return self._estimator.transform(y_copy)\n",
    "    def inverse_transform(self, y):\n",
    "        y_copy = y.copy()\n",
    "        y_reverse = np.exp(self._estimator.inverse_transform(y_copy))\n",
    "        \n",
    "        return y_reverse\n",
    "\n",
    "class InputLogTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self._estimator = StandardScaler()\n",
    "    def fit(self, X):\n",
    "        X_copy = X.copy()\n",
    "        X_copy = log_transform(X_copy)\n",
    "        self._estimator.fit(X_copy)\n",
    "        \n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        X_copy = log_transform(X_copy)\n",
    "        return self._estimator.transform(X_copy)\n",
    "    def inverse_transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        X_reverse = log_inverse(self._estimator.inverse_transform(X_copy))\n",
    "        \n",
    "        return X_reverse\n",
    "    \n",
    "class LDIAModel(nn.Module):\n",
    "    '''\n",
    "    Perceptron Model of variable architecture for hyperparameter tuning\n",
    "    '''\n",
    "    def __init__(self, n_hidden = 1,n_neurons=64,activation=nn.LeakyReLU()):\n",
    "        super().__init__()\n",
    "        self.norms = []\n",
    "        self.layers = []\n",
    "        self.acts = []\n",
    "        self.norm0 = nn.BatchNorm1d(3)\n",
    "        self.layer0 = nn.Linear(3,n_neurons)\n",
    "        for i in range(1,n_hidden+1):\n",
    "            self.norms.append(nn.BatchNorm1d(n_neurons))\n",
    "            self.acts.append(activation)\n",
    "            self.add_module(f\"norm{i}\", self.norms[-1])\n",
    "            self.add_module(f\"act{i}\", self.acts[-1])\n",
    "            if (i != n_hidden):\n",
    "                self.layers.append(nn.Linear(n_neurons, n_neurons))\n",
    "                self.add_module(f\"layer{i}\", self.layers[-1])\n",
    "        self.output = nn.Linear(n_neurons, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "          Forward pass\n",
    "        '''\n",
    "        x = self.layer0(self.norm0(x))\n",
    "        for norm, layer, act in zip(self.norms, self.layers, self.acts):\n",
    "            x = act(layer(norm(x)))\n",
    "        return self.output(x)\n",
    "    \n",
    "def build_neural_network(max_epochs=100, n_hidden=3, n_neurons=32, activation=nn.LeakyReLU(), device=torch.device('cuda'),loss_fn=nn.MSELoss(), optimizer=optim.Adam, lr=1e-2, shuffled=True, batch_size=1024, patience=5, gamma=0.95,valid_ds=None,compiled=False):\n",
    "    return NeuralNetRegressor(\n",
    "    module=LDIAModel,\n",
    "    max_epochs = max_epochs,\n",
    "    module__n_hidden=n_hidden,\n",
    "    module__n_neurons = n_neurons,\n",
    "    module__activation=activation,\n",
    "    device=device,\n",
    "    criterion = loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    optimizer__lr = lr,\n",
    "    iterator_train__shuffle=shuffled,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[('early_stopping', EarlyStopping(patience=patience,monitor='valid_loss')),\n",
    "    ('lr_scheduler', LRScheduler(policy='ExponentialLR',gamma=gamma))],\n",
    "    train_split=predefined_split(valid_ds),\n",
    "    compile = compiled \n",
    ")\n",
    "\n",
    "def make_datasets(X, y, *, train_size=0.8, random_state=42):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=train_size,random_state=random_state)\n",
    "    input_transformer = InputLogTransformer()\n",
    "    output_transformer = OutputLogTransformer()\n",
    "    X_train = input_transformer.fit_transform(X_train)\n",
    "    X_val = input_transformer.transform(X_val)\n",
    "    y_train = output_transformer.fit_transform(y_train)\n",
    "    y_val = output_transformer.transform(y_val)\n",
    "    #train_ds = Dataset(X_train, y_train)\n",
    "    #valid_ds = Dataset(X_val, y_val)\n",
    "    return X_train, y_train, X_val, y_val, input_transformer, output_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5c450b2-c6b7-43e7-9a1c-31928cd27594",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = 30 # ADJUST level of gaussian noise added to outputs\n",
    "mod_type = 'nn'\n",
    "description = mod_type + '_noise-' + str(noise)\n",
    "filename = '../datasets/fuchs_v3-2_seed-5_points_25000_noise_' + str(noise) + '.csv'  # CHANGE TO DESIRED DATA FILE\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "input_list = ['Intensity_(W_cm2)', 'Target_Thickness (um)', 'Focal_Distance_(um)'] # independent variables\n",
    "output_list = ['Max_Proton_Energy_(MeV)', 'Total_Proton_Energy_(MeV)', 'Avg_Proton_Energy_(MeV)',\n",
    "               'Max_Proton_Energy_Exact_(MeV)', 'Total_Proton_Energy_Exact_(MeV)', 'Avg_Proton_Energy_Exact_(MeV)'] # training outputs\n",
    "X = np.array(df[input_list].copy(), dtype=np.float32)\n",
    "y = np.array(df[output_list].copy(), dtype=np.float32)\n",
    "    \n",
    "train_split = 0.8 # Reserve 80% of entire dataset for training\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_split, shuffle = False)\n",
    "y_train = y_train[:, 0:3]\n",
    "\n",
    "pct = 50 # e.g. 10% is 2,000 points training\n",
    "len_df = int(len(X_train)*(pct/100))\n",
    "X_train = X_train[0:len_df]\n",
    "y_train = y_train[0:len_df]\n",
    "\n",
    "X_train_scaled, y_train_scaled, X_val_scaled, y_val_scaled, input_transformer, output_transformer = make_datasets(X_train, y_train, random_state=42)\n",
    "train_ds = Dataset(X_train_scaled, y_train_scaled)\n",
    "valid_ds = Dataset(X_val_scaled, y_val_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7112d0c5-5df0-494b-ac4f-a5b08ad1f3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss      lr     dur\n",
      "-------  ------------  ------------  ------  ------\n",
      "      1        \u001b[36m0.3381\u001b[0m        \u001b[32m0.0654\u001b[0m  0.0010  0.1988\n",
      "      2        \u001b[36m0.0478\u001b[0m        \u001b[32m0.0412\u001b[0m  0.0009  0.1280\n",
      "      3        \u001b[36m0.0428\u001b[0m        0.0413  0.0009  0.1281\n",
      "      4        0.0430        \u001b[32m0.0396\u001b[0m  0.0009  0.1262\n",
      "      5        \u001b[36m0.0414\u001b[0m        \u001b[32m0.0394\u001b[0m  0.0008  0.1245\n",
      "      6        0.0442        \u001b[32m0.0387\u001b[0m  0.0008  0.1242\n",
      "      7        0.0437        \u001b[32m0.0380\u001b[0m  0.0007  0.1252\n",
      "      8        0.0415        0.0404  0.0007  0.1241\n",
      "      9        0.0422        0.0386  0.0007  0.1277\n",
      "     10        0.0414        0.0386  0.0006  0.1244\n",
      "     11        \u001b[36m0.0412\u001b[0m        0.0383  0.0006  0.1258\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "[1.0384319 1.0345069 1.032342 ] 1.044030650891055\n",
      "0.25720128\n",
      "0.26341668\n",
      "0.25660914\n"
     ]
    }
   ],
   "source": [
    "num_inputs = 3\n",
    "num_outputs = 3\n",
    "n_hidden = 3\n",
    "n_neurons = 64\n",
    "train_split = 0.8 # Reserve 80% of entire dataset for training\n",
    "batch_size = 256\n",
    "γ = 0.95\n",
    "max_epochs = 100\n",
    "lr=1e-3\n",
    "\n",
    "nn_model = build_neural_network(max_epochs=max_epochs, \n",
    "                                 n_hidden=n_hidden, \n",
    "                                 n_neurons=n_neurons, \n",
    "                                 activation=nn.LeakyReLU(), \n",
    "                                 device=device,loss_fn=nn.MSELoss(), \n",
    "                                 optimizer=optim.Adam, \n",
    "                                 lr=lr, \n",
    "                                 shuffled=True, \n",
    "                                 batch_size=batch_size, \n",
    "                                 patience=5, \n",
    "                                 gamma=γ, \n",
    "                                 valid_ds=valid_ds)\n",
    "    \n",
    "nn_model.fit(train_ds, y=None)\n",
    "\n",
    "y_train_predict = nn_model.predict(X_train_scaled)\n",
    "y_train_predict_unscaled = output_transformer.inverse_transform(y_train_predict)\n",
    "\n",
    "# Corrections due to Log Scaling\n",
    "y_train = output_transformer.inverse_transform(y_train_scaled)\n",
    "correction_factor = np.mean(y_train/y_train_predict_unscaled, axis=0) \n",
    "y_train_predict_corrected = y_train_predict_unscaled*correction_factor\n",
    "print(correction_factor, np.sqrt(1+(noise/100)**2))\n",
    "for j in range(num_outputs):\n",
    "    print(mape(y_train[:, j], y_train_predict_corrected[:, j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87ee8a0c-4340-499c-adc6-cdb9c619f39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X):\n",
    "    X_scaled = input_transformer.transform(X.copy().reshape(-1, 3))\n",
    "    energies = output_transformer.inverse_transform(nn_model.predict(X_scaled))\n",
    "    E_max = energies[:, 0]\n",
    "    E_tot = energies[:, 1]\n",
    "    E_avg = energies[:, 2]\n",
    "\n",
    "    return E_max, E_tot, E_avg\n",
    "\n",
    "def generate_random_points(bounds, n):\n",
    "    np.random.seed(0)\n",
    "    points = []\n",
    "    for bound in bounds:\n",
    "        points.append(np.random.uniform(bound[0], bound[1], n))\n",
    "    return np.array(points, dtype=np.float32).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01815a08-6e05-4a4a-922c-c756035be3c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intensity</th>\n",
       "      <th>Thickness</th>\n",
       "      <th>Offset</th>\n",
       "      <th>E Max</th>\n",
       "      <th>E Tot</th>\n",
       "      <th>E Avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000e+19</td>\n",
       "      <td>5.584942</td>\n",
       "      <td>2.177699</td>\n",
       "      <td>0.326697</td>\n",
       "      <td>84157568.0</td>\n",
       "      <td>0.081434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000e+19</td>\n",
       "      <td>9.088420</td>\n",
       "      <td>5.872497</td>\n",
       "      <td>0.125710</td>\n",
       "      <td>28633386.0</td>\n",
       "      <td>0.034329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000e+19</td>\n",
       "      <td>5.272768</td>\n",
       "      <td>9.876133</td>\n",
       "      <td>0.174977</td>\n",
       "      <td>38419624.0</td>\n",
       "      <td>0.045561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000e+19</td>\n",
       "      <td>1.458265</td>\n",
       "      <td>9.508671</td>\n",
       "      <td>0.521847</td>\n",
       "      <td>124839792.0</td>\n",
       "      <td>0.109516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000e+19</td>\n",
       "      <td>5.512029</td>\n",
       "      <td>3.429715</td>\n",
       "      <td>0.312105</td>\n",
       "      <td>79200744.0</td>\n",
       "      <td>0.078045</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Intensity  Thickness    Offset     E Max        E Tot     E Avg\n",
       "0  1.000000e+19   5.584942  2.177699  0.326697   84157568.0  0.081434\n",
       "1  1.000000e+19   9.088420  5.872497  0.125710   28633386.0  0.034329\n",
       "2  1.000000e+19   5.272768  9.876133  0.174977   38419624.0  0.045561\n",
       "3  1.000000e+19   1.458265  9.508671  0.521847  124839792.0  0.109516\n",
       "4  1.000000e+19   5.512029  3.429715  0.312105   79200744.0  0.078045"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bounds = [(1e19, 1e19), (0.5, 10.0), (0, 10.0)]\n",
    "n_points = 100000\n",
    "points = generate_random_points(bounds, n_points)\n",
    "Emax, Etot, Eavg = model(points)\n",
    "output_df = pd.DataFrame(columns=['Intensity', 'Thickness', 'Offset', 'E Max', 'E Tot', 'E Avg'])\n",
    "output_df['Intensity'] = points[:, 0]\n",
    "output_df['Thickness'] = points[:, 1]\n",
    "output_df['Offset'] = points[:, 2]\n",
    "output_df['E Max'] = Emax\n",
    "output_df['E Tot'] = Etot\n",
    "output_df['E Avg'] = Eavg\n",
    "output_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91ed3bc7-0b7a-42fe-bfdd-11c001cd6161",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.to_csv('predictions_dfs/{}_noise={}_train_pts={}.csv'.format(mod_type, noise, len_df), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f0eb9a-58e9-4e34-8c93-38e34e118012",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
